# Moving Recommendation Generation “Offline”  
Design | Implementation | Best-Practice Guide  
CineAI Movie Recommendation App

--------------------------------------------------------------------
## 1  Problem Statement

Today the `UnifiedAIService` runs inside API routes (or even React Server Components) and makes multiple Claude/LLM calls at request time.  
Resulting pain points:

1. **User-visible latency** – Up to 3-5 s on first page load while AI finishes.  
2. **Bursty token spend** – Peak traffic → peak Anthropic bill.  
3. **Limited algorithm depth** – We avoid multi-pass or large-context prompts because they would make the request timeout.  
4. **Inefficient recomputation** – The same user/movie pair can be analysed repeatedly across sessions.

--------------------------------------------------------------------
## 2  Goal / Success Criteria

• Page requests must become DB-only (≤ 200 ms).  
• AI pipelines may run minutes or hours before they’re needed.  
• System scales linearly with users, not page views.  
• We preserve the ability to refresh quickly after the user takes an action (rating, watch-list add).

--------------------------------------------------------------------
## 3  Solution Overview

1. **Move heavy work to background jobs** (cron or event-driven Edge Functions).  
2. **Persist results** in `recommendations` (already in schema) and specialised analysis tables.  
3. **Serve UI** by reading pre-computed rows; fall back to real-time generation only if nothing exists.  
4. **Trigger refresh** when user behaviour changes (ratings, preferences).

--------------------------------------------------------------------
## 4  High-Level Architecture

```text
                ┌────────────┐
  user action ──►   RLS DB    ──► TRIGGER (ratings insert) ┐
                └────────────┘                             │queue
                                                           ▼
                              nightly cron   ┌──────────────────────────┐
                              --------------►│ recommendation-job edge │
                                              │  (Supabase function)    │
                                              └──────────────────────────┘
                                                         │ upsert
                                                         ▼
                       ┌───────────────────────────────────────────────┐
 GET /dashboard        │ recommendations (user_id, movie_id, score..) │
──────────────────────►│ + thematic / emotional / style aux tables    │
                       └───────────────────────────────────────────────┘
```

--------------------------------------------------------------------
## 5  Implementation Steps

### 5.1 Schema Tweaks  

```sql
alter table recommendations
  add column if not exists last_generated_at timestamptz,
  add column if not exists status text default 'ready';
```

Optional: small `rec_refresh_queue(user_id uuid primary key, queued_at timestamptz)` if you prefer queue table over NOTIFY.

### 5.2 Server-Side Helper  

`src/jobs/generateRecommendations.ts`

```ts
import { getRecommendations } from '@/lib/ai/unified-ai-service'
import { createClient } from '@supabase/supabase-js'

const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
)

export async function generateForUser(userId: string, limit = 50) {
  const result = await getRecommendations({ userId, context: { limit } })

  const rows = result.movies.map((m, i) => ({
    user_id: userId,
    movie_id: m.id,
    score: result.insights?.confidence ?? 0.7,
    reason: (result.explanations?.get(m.id) ?? {}).primaryReasons?.[0] ?? 'AI',
    created_at: new Date().toISOString(),
  }))

  await supabase.from('recommendations')
    .upsert(rows, { onConflict: 'user_id,movie_id' })
}
```

### 5.3 Edge Function (Supabase)  

`supabase/functions/recommendation-job/index.ts`

```ts
import { serve } from 'std/server'
import { generateForUser } from '@/jobs/generateRecommendations'

serve(async (req) => {
  const { userId } = await req.json()
  if (!userId) return new Response('Bad Request', { status: 400 })

  try {
    await generateForUser(userId)
    return new Response('ok')
  } catch (e) {
    return new Response(`Error ${e}`, { status: 500 })
  }
})
```

Schedule it:

```bash
supabase functions deploy recommendation-job --project-ref movie
supabase functions schedule recommendation-job --schedule "0 * * * *"   # hourly
```

### 5.4 Queue & Event Triggers  

```sql
create or replace function enqueue_rec_refresh() returns trigger as $$
begin
  insert into rec_refresh_queue values (new.user_id, now())
  on conflict (user_id) do update set queued_at = now();
  return new;
end;
$$ language plpgsql;

create trigger ratings_refresh
after insert or update on ratings
for each row execute function enqueue_rec_refresh();
```

A lightweight “queue-drain” edge function runs every 5 min, pops N user_ids and calls `generateForUser`.

### 5.5 Frontend Adjustments  

```ts
const { data: recs } = await supabase
  .from('recommendations')
  .select('score, reason, movies (*)')
  .eq('user_id', user.id)
  .order('score', { ascending: false })
  .limit(20)

if (recs.length === 0) {
  // fallback
  await fetch('/api/recommendations?algorithm=smart')  
}
```

--------------------------------------------------------------------
## 6  Operational Considerations

| Topic | Recommendation |
|-------|----------------|
| **Monitoring** | Add row count / age checks to `production-monitor.ts`. Alert if `last_generated_at` > 6 h. |
| **Rate limits** | Batch AI calls: 25 users per minute × 10 movies each ≈ 250 Claude calls/hour → within free tier. |
| **Scaling up** | Increase cron frequency or spin multiple workers; use `status = "in_progress"` lock flag to avoid duplicates. |
| **Cold start** | Run a one-time script that iterates all existing users. |
| **Data retention** | Keep only top-200 per user or purge rows older than 30 days. |

--------------------------------------------------------------------
## 7  Pros & Cons

| Pros | Cons / Mitigations |
|------|-------------------|
| Sub-100 ms UI responses | Recommendations can be up to 1 h “old”. Adjustable via cron interval |
| Predictable LLM spend | Slightly higher complexity (jobs, queues) |
| Allows deeper models / embeddings | Extra storage for recommendation rows |
| Works offline (mobile) | Need fall-back path for new users (handled) |

--------------------------------------------------------------------
## 8  Future Enhancements

1. **Vector similarity pre-selection** – Store `storyline_embedding` and use pgvector to shortlist candidates before Claude scoring.  
2. **Realtime micro-refresh** – For “You liked *Inception*” show 3 instant picks by local similarity while full job runs.  
3. **A/B rotate algorithms** – Persist `algorithm` used in each row for analytics.  
4. **Edge-cached JSON** – Convert recommendations to static JSON files on edge CDN for fully serverless read path.

--------------------------------------------------------------------
### TL;DR

Move `UnifiedAIService.getRecommendations()` into an hourly Supabase Edge Function (plus a ratings-triggered micro-queue), store the outcome in `recommendations`, read that table in the UI, and keep the existing API route only as an initial fallback.  
This cuts perceived latency to near-zero and gives you headroom for richer, slower AI pipelines without exploding costs.